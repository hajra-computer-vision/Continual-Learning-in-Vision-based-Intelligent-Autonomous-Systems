# Continual Learning in Vision-based Intelligent Autonomous Systems
## Benchmarks
Tables below list benchmarks used to assess continual learning (CL) methods within the core capabilities of intelligent autonomous systems (IAS).

**Table 1:** Benchmark Datasets for Continual Learning in 3D Perception
| **Datasets**                                     | **Modality**                                 | **Total Classes**            | **Total Instances**              | **Train**                    | **Test**     | **Continual Learning Evaluation Protocol**                                                                                                                                                                                                                                                                                  | **Download Link**                                                                                                                     |
| :----------------------------------------------- | :------------------------------------------- | :--------------------------- | :------------------------------- | :--------------------------- | :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------ |
| ModelNet  <br>  \[@ref66]                    | 3D CAD models                                | 40                           | 12311                            | 9843                         | 2468         | It follows an IL setup, where training starts with a subset of 40 object categories, and new classes are introduced sequentially across 10 incremental states. At each step, 4 new classes are added, and the model must classify both past and new classes with a memory buffer of 800 exemplars being used. \[@ref9]. | https://modelnet.cs.princeton.edu/                                                                                  |
| ShapeNet  <br>  \\cite{ref66}                    | 3D CAD models                                | 53                           | 1000 samples stored as exemplars | –                            | –            | It follows an IL setup, beginning with a subset of 53 object categories. New classes are introduced progressively over 9 incremental states, with 6 additional classes added at each step, and a memory buffer of 1000 exemplars being maintained. \\cite{ref9}.                                                            | https://shapenet.org/                                                                                               |
| ScanNet  <br>  \\cite{ref67}                     | RGB-D + 3D Point clouds                      | 17                           | 15476                            | 12060                        | 3416         | The training starts with a subset of 17 object categories, with new classes being introduced over 9 incremental states, with 2 additional classes added at each step, and a memory buffer of 600 exemplars. \\cite{ref9}.                                                                                                   | https://github.com/ScanNet/ScanNet                                                                                  |
| S3DIS  <br>  \\cite{ref68}                       | 3D Point cloud scenes                        | 13                           | 271                              | –                            | –            | For open set segmentation, the base model is being trained with 11 known classes and 2 novel classes. \\cite{ref22}.                                                                                                                                                                                                        | https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy\_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&w=1        |
| MNIST  <br>  \\cite{ref10}                       | 3D Point clouds                              | 10                           | 70000                            | –                            | –            | Being used for both class-IL and task IL scenarios, following the standard Split MNIST protocol \\cite{ref65}, where the dataset is divided into 5 tasks with two classes per task. \\cite{ref10}.                                                                                                                          | https://www.kaggle.com/datasets/daavoo/3d-mnist                                                                     |
| ICL-NUIM livingroom  <br>  \\cite{ref69}         | Depth maps + 3D models                       | –                            | –                                | 2 Indoor scenes              | –            | Used for quantitative and qualitative evaluation of the proposed continual neural mapping method, specifically for testing scene geometry approximation. \\cite{ref30}.                                                                                                                                                     | https://www.doc.ic.ac.uk/~ahanda/VaFRIC/living\_room.html                                                           |
| TUM  <br>  \\cite{ref70}                         | RGB-D scans                                  | –                            | –                                | 39 Indoor sequences          | –            | Employed for additional qualitative evaluation of neural mapping. \\cite{ref30}.                                                                                                                                                                                                                                            | https://cvg.cit.tum.de/data/datasets/rgbd-dataset                                                                   |
| CRLMaze  <br>  \\cite{ref35}                     | Visual data (3D environments with RGB input) | 4 scenarios (2 classes each) | 20                               | 300–600 episodes             | 100 episodes | A benchmark to evaluate the continual reinforcement learning approaches, including a new model-free strategy called CRL-Unsup, across four scenarios with incremental difficulty and environmental changes. \\cite{ref35}.                                                                                                  | https://github.com/vlomonaco/crlmaze                                                                                |
| Semantic-\\allowbreak KITTI  <br>  \\cite{ref71} | 3D Point clouds                              | 28                           | 43552                            | Sequences 00–07, 09–10       | Sequence 08  | Being used in a novel class discovery setup where it is split into 4 parts, each with novel and base classes. \\cite{ref18}.                                                                                                                                                                                                | https://www.semantic-kitti.org/                                                                                     |
| Semantic- <br>  POSS <br>  \\cite{ref72}         | 3D Point clouds                              | 13                           | 2988                             | Sequences 00, 01, 02, 04, 05 | Sequence 03  | For novel class discovery, the dataset is split into 4 parts, each with novel and base classes. \\cite{ref18}.                                                                                                                                                                                                              | http://www.poss.pku.edu.cn/semanticposs.html                                                                        |
| Waymo- <br>  Open  <br>  \\cite{ref73}           | 3D Point clouds                              | 3                            | 1748                             | 789202                       | 757          | Has been used to evaluate the efficiency of the proposed diversity knowledge distillation framework by testing its performance on student detectors using both 20% and 100% of the dataset for training. \\cite{ref29}.                                                                                                     | https://waymo.com/open/download                                                                                     |
| DALES  <br>  \\cite{ref74}                       | 3D Point clouds                              | 8                            | 2100 blocks                      | –                            | –            | Being used in domain incremental and class-IL tasks along with other datasets such as ISPRS \\cite{ref76}, H3D \\cite{ref77}, and the Cambridge region (CAM) of the Sen-satUrban dataset \\cite{ref75}. \\cite{ref21}.                                                                                                      | https://udayton.edu/engineering/research/centers/vision\_lab/research/was\_data\_analysis\_and\_processing/dale.php |
| 7Scenes  <br>  \\cite{ref78}                     | RGB-D images                                 | 7                            | –                                | –                            | –            | Employed in continual visual localization besides other datasets such as 12Scenes \\cite{ref79}, and 19Scenes. \\cite{ref36}                                                                                                                                                                                                | https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/                                            |
| Toys-200  <br>  \\cite{ref12}                    | 3D Models                                    | –                            | 200                              | –                            | –            | Experiments involve IL on Toys-200 (2000 exposures, 600 exemplars), CRIB-Toys-50 (50 objects, 500 exposures), CRIB-ShapeNet-20 (categorization tasks), and CIFAR-20\. \\cite{ref12}.                                                                                                                                        | https://iolfcv.github.io/                                                                                           |
| nuScenes  <br>  \\cite{ref80}                    | 3D Point clouds                              | 23                           | –                                | –                            | –            | Training samples are divided into 3 regions, containing 60,960, 40,960, and 73,780 samples, respectively. The model is first trained on Region 1, then incrementally extended, finally to Regions 1, 2 \\& 3, with a consistent improvement as more regions are added. \\cite{ref37}.                                       | https://www.nuscenes.org/                                                                                           |

**Table: 2** Publicly available datasets for continual learning in observation and analysis
| **Datasets** | **Description**    | **Continual Learning Evaluation Protocol** | **Download Link** |
| :--------------------- | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- |
| UCF-101 \\cite{soomro2012ucf101}                 | Human action recognition dataset that includes 13,320 instances across 101 action categories. The data is divided into two splits: 70% training and 30% testing.                                                          | The initial training is performed on 51 classes, and the subsequent 50 classes are structured into IL tasks.\\cite{cheng2024stsp,feng\_spatiotemporal\_2024}                                                                                                                                | https://www.crcv.ucf.edu/data/UCF101/UCF101\.rar                                         |
| Kinetics-100 \\cite{kay2017kinetics}             | Kinetics is a YouTube-type video dataset that contains 400 human action categories, including 240,000 training videos and 20,000 validation videos.                                                                       | \\cite{feng\_spatiotemporal\_2024} randomly selects 100 classes and splits them into 40 incremental classes and 60 base classes. Whereas \\cite{zhao\_when\_2021} randomly selects a subset of 40 classes from Kinetics, further splitting into 20 base and 20 incremental classes.         | https://github.com/cvdfoundation/kinetics-dataset                                        |
| HMDB-51 \\cite{kuehne2011hmdb}                   | HMDB-51 includes 7000 video clips distributed among 51 action categories. Videos are collected from movies and public databases such as YouTube and Google.                                                               | The base model is trained on 26 classes, and the remaining 25 are used in IL. \\cite{pei\_space-time\_2023}                                                                                                                                                                                 | https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\#Downloads |
| NOLA \\cite{doshi\_rethinking\_2022}             | Anomaly detection dataset consists of 110 training video segments and 50 test video segments. These are recorded from a famous street in New Orleans, Louisiana, USA.                                                     | Training videos are divided into 11 splits. Where 1 split is used for initial training, and the rest 10 splits are used in IL. \\cite{doshi\_rethinking\_2022}                                                                                                                              | https://noladataset.s3\.us-west-2.amazonaws.com/NOLA.zip                                 |
| PKU-MMD \\cite{liu2017pku}                       | Human action 3D skeleton dataset, including 51 action classes and 21,545 labelled action instances performed by 66 distinct subjects.                                                                                     | In the IL setup, human actions are divided into multiple tasks. 41 classes are used for pre-training, and 10 unseen classes are learned continually.\\cite{li\_else-net\_2021}                                                                                                              | https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html                                  |
| NTU RGB+D \\cite{shahroudy2016ntu}               | NTU RGB+D is a large-scale 3D skeleton action recognition dataset introduced in two versions. The first version consists of 56,880 videos divided into 60 actions, and later expanded to 120 classes and 113,945 samples. | Data is divided into 6 tasks, where each consists of 10 action classes. \\cite{mosconi\_mask\_2024}                                                                                                                                                                                         | https://rose1\.ntu.edu.sg/dataset/actionRecognition/                                     |
| Something-Something V2\\cite{goyal2017something} | Includes 220,000 videos of 174 basic actions using everyday objects.                                                                                                                                                      | The base model is trained on 84 action classes, while the remaining actions are organized into IL groups of 10 and 5 classes. \\cite{cheng2024stsp}                                                                                                                                         | https://www.qualcomm.com/developer/software/something-something-v-2-dataset/downloads    |
| ActivityNet \\cite{caba2015activitynet}          | Action recognition dataset that includes 20,000 YouTube video clips divided into 200 action classes.                                                                                                                      | The Dataset is organized in 10 tasks, where each task includes 20 actions, making a class-incremental setting. \\cite{villa\_pivot\_2023,Fu2024DecoupledPT}                                                                                                                                 | http://activity-net.org/download.html                                                    |
| EPIC-Kitchens-100 \\cite{damen2020epic}          | Egocentric action recognition dataset that records 90,000 video segments, which are human activities in the kitchen. Each action is labeled as a combination of a verb and a noun, making 97 action classes.              | 97 activities are divided into 10 non-overlapping tasks in a class incremental setup. \\cite{Fu2024DecoupledPT}                                                                                                                                                                             | https://epic-kitchens.github.io/2025                                                     |
| UESTC-MMEA-CL \\cite{xu2023towards}              | Multi-modal action dataset that contains 200 synchronized egocentric video streams, acceleration sensing sequences, and gyroscope sensing sequences. These sequences are labeled as 32 human daily activities.            | Model is pre-trained on 16 base classes, and the remaining 16 are learned in an incremental fashion. \\cite{he2024continual,lu\_video\_2024}                                                                                                                                                | https://ivipclab.github.io/publication\_uestc-mmea-cl/mmea-cl/                           |
| LiveFood \\cite{pei\_2024}                       | Video highlight detection dataset including 5,100 gourmet videos. They are divided into 4928 videos for training and 261 videos for testing.                                                                              | In the domain incremental setup, 4928 videos are randomly split into four domains, having 3380, 854, 393, and 113 videos. Each domain is defined as presentation, presentation, eating, presentation, eating, ingredients, and presentation, eating, ingredients, cooking.\\cite{pei\_2024} | Not open source                                                                                            |
| vClimb \\cite{vCLIMB}                            | A benchmark developed using UCF101, Kinetics, and ActivityNet action recognition datasets                                                                                                                                 | Includes 8 splits for class IL, where each split contains 10 or 20 tasks. \\cite{vCLIMB}                                                                                                                                                                                                    | https://github.com/ojedaf/vCLIMB\_Benchmark                                              |

**Table: 3** Publicly available datasets for continual learning in navigation

| **Datasets** | **Description**    | **Continual Learning Evaluation Protocol** | **Download Link** |
| :------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- |
| SODA10M \\cite{han2021soda10m}              | 10 million unlabelled images \\& 20,000 labelled images. 6 object classes. Training (22,249 objects in 3,841 images), Validation (6,305 objects in 1,590 images), Test (69,881 objects in 10,000 images). | Average accuracy of each class measured at specific points during the data stream. Based on the average mAP of each task at the end of training, reported at an IOU level of 0\.5.               | https://soda-2d.github.io/                                               |
| KITTI 3D \\cite{geiger2012we}               | 7481 training images and 7518 test images as well as the corresponding point clouds, comprising a total of 80,256 labeled objects.                                                                        | Divided into 5 tasks, which are merged incrementally. Evaluation metrics include 3D average precision (AP) with specific IoU thresholds. Performance is measured using mAP, BWT, FWT, and R-BWT. | https://www.cvlibs.net/datasets/kitti/eval\_object.php?obj\_benchmark=3d |
| Cityscapes \\cite{Cordts2016Cityscapes}     | 5,000 images with high quality annotations. 20,000 images with coarse annotations. 50 different cities and 30 classes.                                                                                    | Each selected class is treated as a novel class. Experiments were conducted separately for each class to simulate the identification of one novel class at a time.                               | https://www.cityscapes-dataset.com/                                      |
| BDD100K \\cite{yu2020bdd100k}               | Largest driving video dataset with 100K videos and 10 tasks.                                                                                                                                              | Each selected class is treated as a novel class. Experiments were conducted separately for each class to simulate the identification of one novel class at a time.                               | https://paperswithcode.com/dataset/bdd100k                               |
| INTERACTION \\cite{zhan2019interaction}     | Consists of actual traffic data collected through highways, intersections, and roundabouts. Includes trajectory data of traffic participants and map information.                                         | Multiple trajectory prediction tasks are constructed corresponding to different environments. Metrics used include Final Displacement Error (FDE) and Missing Rate (MR).                         | http://interaction-dataset.com/                                          |
| nuScenes \\cite{caesar2020nuscenes}         | 450 scenes. Includes 400 scenes for pre-training and 50 scenes for sequential updates.                                                                                                                    | Data is continuously streamed and divided into adaptation and evaluation sets. Performance is measured using average displacement error (ADE).                                                   | https://www.nuscenes.org/                                                |
| DrivingStereo \\cite{yang2019drivingstereo} | Contains over 180,000 images covering a diverse set of driving scenarios. High-quality labels of disparity are produced by a model-guided filtering strategy from multi-frame LiDAR points.               | Y Used with four weather conditions (cloudy, foggy, rainy, sunny) to construct a task sequence. Metrics used include Final Average Error (FAE) and Backward Transfer (BWT).                      | https://drivingstereo-dataset.github.io/                                 |

**Table: 4** Benchmark datasets for continual learning in interaction and manipulation

| **Datasets** | **Description**    | **Continual Learning Evaluation Protocol** | **Download Link** |
| :---------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------- |
| KITTI \\cite{Geiger2013IJRR}                                | Autonomous driving dataset for 3D object tracking, SLAM, and scene understanding with LiDAR, GPS, and stereo camera data.                                                                                                                              | Liu et al. \\cite{liu2024semi} structure training on a subset of sequences, incrementally adapting to new ones to assess knowledge retention. Vödisch et al. \\cite{V\_disch\_2023} train on KITTI 0,1,2,8,9 and incrementally adapt to 4,5,6,7,10 using online learning.                                                                   | https://www.cvlibs.net/datasets/kitti/                                       |
| CORe50 \\cite{lomonaco2017core50}                           | Streaming learning benchmark with 50 object categories from 11 domains, designed for continuous object recognition.                                                                                                                                    | \\cite{nenakhov2023continuous}'s setup consists of objectsof 10 classes with 5 instances of each object. Each instance has300 images.                                                                                                                                                                                                       | https://vlomonaco.github.io/core50/                                          |
| nuScenes \\cite{caesar2020nuscenes}                         | Large-scale autonomous driving dataset with 1000 scenes containing 3D object annotations and HD maps.                                                                                                                                                  | The initial training is performed on 60,960 samples from nuScenes, with subsequent 40,960 and 73,780 samples introduced incrementally \\cite{Karlsson\_2023}.                                                                                                                                                                               | https://www.nuscenes.org/                                                    |
| Human3\.6M \\cite{6682899}                                  | Large-scale 3D human motion capture dataset with 15 activities performed by 11 subjects.                                                                                                                                                               | Out of the 15 activities \\cite{9811906Xu2022} uses subsets 1, 6, 7, 8, 9 for the training set, subset 11 for the validation set, and subset 5 for the testing set, which contain 172K, 26K, and 46K samples respectively                                                                                                                   | http://vision.imar.ro/human3\.6m/                                            |
| BIWI Walking Pedestrians \\cite{5459260}                    | Walking pedestrians in busy scenarios from a bird eye view, manually annotated dataset.                                                                                                                                                                | Training on pedestrian trajectories from 3 environments (Square, Obstacle, Hall), collecting T = 200s of data per task. \\cite{knoedler2022improving}.                                                                                                                                                                                      | https://icu.ee.ethz.ch/research/datsets.html                                 |
| Stanford Drone \\cite{robicquet2016learning}                | Complex trajectory dataset capturing diverse interactions includes 6 different scenes, among which observed objects include pedestrians, vehicles, and skateboards.                                                                                    | Training across four scenes, using continual learning to adapt to different environments.\\cite{YANG2022110022}.                                                                                                                                                                                                                            | https://cvgl.stanford.edu/projects/uav\_data/                                |
| TartanAir \\cite{wang2020tartanair}                         | Large-scale synthetic dataset for SLAM with varied environmental conditions, with 30 photo-realistic simulation environments in the Unreal Engine.                                                                                                     | Initial training is done on five scenes from TartanAir, with 80% of sequences used for training and the rest for IL. \\cite{9811658roboticsdetection1}.                                                                                                                                                                                     | https://theairlab.org/tartanair-dataset/                                     |
| Oxford RobotCar \\cite{RobotCarDatasetIJRR}                 | The Dataset comprises over 100 repetitions of a fixed route in Oxford, UK, collected over a year. It captures varying weather, traffic, and pedestrian conditions, as well as long-term changes like construction and roadworks.                       | The initial training is conducted on two sequences per environment (sun, overcast, night). Loop closures define frames within 10m distance and 15° yaw difference  \\cite{9811658roboticsdetection1} and \\cite{V\_disch\_2023} perform initial training on Oxford RobotCar sequences from August 12, 2015, covering a 4,000-frame segment. | https://robotcar-dataset.robots.ox.ac.uk/                                    |
| Cityscapes \\cite{Cordts2016Cityscapes}                     | A large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5000 frames in addition to a larger set of 20000 weakly annotated frames. | The initial training is performed on the official Cityscapes training split for 25 epochs, with images resized to 192×640 pixels and dynamic objects masked using YOLOv5m. Continual learning adapts to new urban environments. \\cite{V\_disch\_2023}.                                                                                     | https://www.cityscapes-dataset.com/                                          |
| Cornell Grasping \\cite{5980145}                            | Robotic grasp detection dataset with RGB-D images.                                                                                                                                                                                                     | The model is initially trained on 90% of the Cornell Grasping Dataset, with images augmented via random rotation and zoom. Continual learning is applied using a dual-memory recurrent network \\cite{SANTHAKUMAR2022167}.                                                                                                                  | https://pr.cs.cornell.edu/grasping/rect\_data/data.php                       |
| LASA Handwriting Dataset \\cite{khansari-zadeh2011learning} | Contains 2D handwriting motions recorded on a Tablet-PC, with 7 demonstrations per pattern converging to a common final point. It includes 30 motions: 26 single patterns and 4 multi-pattern models.                                                  | Continual learning using demonstration of a task in a sequence of 1000 2-D points. The alphabets are arranged in the order of their names and trained sequentially in models. \\cite{auddy2023continuallearningdemonstrationrobotics}.                                                                                                      | https://cs.stanford.edu/people/khansari/download.html                        |
| UCI Wall-Following \\cite{freire2009wall}                   | Contains robot wall navigation data in a clockwise direction for 4 rounds, using 24 ultrasound sensors around its 'waist.' The data includes raw sensor measurements and corresponding class labels.                                                   | \\cite{madi2024new} train their model on 5,456 sensor readings from the dataset, with data collected at 9 samples per second using 24 ultrasonic sensors, new instances are added and replaced dynamically to assist in Continual Learning.                                                                                                 | https://archive.ics.uci.edu/dataset/194/wall+following+robot+navigation+data |
